{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3c3b1c46-9f5c-41c1-9101-85db8709ec0d",
      "metadata": {
        "id": "3c3b1c46-9f5c-41c1-9101-85db8709ec0d"
      },
      "outputs": [],
      "source": [
        "# Copyright (c) Meta Platforms, Inc. and affiliates."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6e7a0db5-7f04-4845-8b11-684fe6e9f7f2",
      "metadata": {
        "id": "6e7a0db5-7f04-4845-8b11-684fe6e9f7f2"
      },
      "source": [
        "# Video segmentation with SAM 2"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "73ba7875-35e5-478b-b8ba-4b48e121dec7",
      "metadata": {
        "id": "73ba7875-35e5-478b-b8ba-4b48e121dec7"
      },
      "source": [
        "This notebook shows how to use SAM 2 for interactive segmentation in videos. It will cover the following:\n",
        "\n",
        "- adding clicks (or box) on a frame to get and refine _masklets_ (spatio-temporal masks)\n",
        "- propagating clicks (or box) to get _masklets_ throughout the video\n",
        "- segmenting and tracking multiple objects at the same time\n",
        "\n",
        "We use the terms _segment_ or _mask_ to refer to the model prediction for an object on a single frame, and _masklet_ to refer to the spatio-temporal masks across the entire video."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a887b90f-6576-4ef8-964e-76d3a156ccb6",
      "metadata": {
        "id": "a887b90f-6576-4ef8-964e-76d3a156ccb6"
      },
      "source": [
        "<a target=\"_blank\" href=\"https://colab.research.google.com/github/facebookresearch/sam2/blob/main/notebooks/video_predictor_example.ipynb\">\n",
        "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
        "</a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "26616201-06df-435b-98fd-ad17c373bb4a",
      "metadata": {
        "id": "26616201-06df-435b-98fd-ad17c373bb4a"
      },
      "source": [
        "## Environment Set-up"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8491a127-4c01-48f5-9dc5-f148a9417fdf",
      "metadata": {
        "id": "8491a127-4c01-48f5-9dc5-f148a9417fdf"
      },
      "source": [
        "If running locally using jupyter, first install `sam2` in your environment using the [installation instructions](https://github.com/facebookresearch/sam2#installation) in the repository.\n",
        "\n",
        "If running from Google Colab, set `using_colab=True` below and run the cell. In Colab, be sure to select 'GPU' under 'Edit'->'Notebook Settings'->'Hardware accelerator'. Note that it's recommended to use **A100 or L4 GPUs when running in Colab** (T4 GPUs might also work, but could be slow and might run out of memory in some cases)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f74c53be-aab1-46b9-8c0b-068b52ef5948",
      "metadata": {
        "id": "f74c53be-aab1-46b9-8c0b-068b52ef5948"
      },
      "outputs": [],
      "source": [
        "using_colab = True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d824a4b2-71f3-4da3-bfc7-3249625e6730",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d824a4b2-71f3-4da3-bfc7-3249625e6730",
        "outputId": "8ee39607-b340-4aec-904b-72eb08ff1f3f"
      },
      "outputs": [],
      "source": [
        "if using_colab:\n",
        "    import torch\n",
        "    import torchvision\n",
        "    print(\"PyTorch version:\", torch.__version__)\n",
        "    print(\"Torchvision version:\", torchvision.__version__)\n",
        "    print(\"CUDA is available:\", torch.cuda.is_available())\n",
        "    import sys\n",
        "    !{sys.executable} -m pip install opencv-python matplotlib\n",
        "    !{sys.executable} -m pip install 'git+https://github.com/facebookresearch/sam2.git'\n",
        "\n",
        "    !mkdir -p videos\n",
        "    !wget -P videos https://dl.fbaipublicfiles.com/segment_anything_2/assets/bedroom.zip\n",
        "    !unzip -d videos videos/bedroom.zip\n",
        "\n",
        "    !mkdir -p ../checkpoints/\n",
        "    !wget -P ../checkpoints/ https://dl.fbaipublicfiles.com/segment_anything_2/092824/sam2.1_hiera_large.pt"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "22e6aa9d-487f-4207-b657-8cff0902343e",
      "metadata": {
        "id": "22e6aa9d-487f-4207-b657-8cff0902343e"
      },
      "source": [
        "## Set-up"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e5318a85-5bf7-4880-b2b3-15e4db24d796",
      "metadata": {
        "id": "e5318a85-5bf7-4880-b2b3-15e4db24d796"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "# if using Apple MPS, fall back to CPU for unsupported ops\n",
        "os.environ[\"PYTORCH_ENABLE_MPS_FALLBACK\"] = \"1\"\n",
        "import numpy as np\n",
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "08ba49d8-8c22-4eba-a2ab-46eee839287f",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "08ba49d8-8c22-4eba-a2ab-46eee839287f",
        "outputId": "5923dfc4-aa5c-4914-ca8f-479363b70948"
      },
      "outputs": [],
      "source": [
        "# select the device for computation\n",
        "if torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda\")\n",
        "elif torch.backends.mps.is_available():\n",
        "    device = torch.device(\"mps\")\n",
        "else:\n",
        "    device = torch.device(\"cpu\")\n",
        "print(f\"using device: {device}\")\n",
        "\n",
        "if device.type == \"cuda\":\n",
        "    # use bfloat16 for the entire notebook\n",
        "    torch.autocast(\"cuda\", dtype=torch.bfloat16).__enter__()\n",
        "    # turn on tfloat32 for Ampere GPUs (https://pytorch.org/docs/stable/notes/cuda.html#tensorfloat-32-tf32-on-ampere-devices)\n",
        "    if torch.cuda.get_device_properties(0).major >= 8:\n",
        "        torch.backends.cuda.matmul.allow_tf32 = True\n",
        "        torch.backends.cudnn.allow_tf32 = True\n",
        "elif device.type == \"mps\":\n",
        "    print(\n",
        "        \"\\nSupport for MPS devices is preliminary. SAM 2 is trained with CUDA and might \"\n",
        "        \"give numerically different outputs and sometimes degraded performance on MPS. \"\n",
        "        \"See e.g. https://github.com/pytorch/pytorch/issues/84936 for a discussion.\"\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ae8e0779-751f-4224-9b04-ed0f0b406500",
      "metadata": {
        "id": "ae8e0779-751f-4224-9b04-ed0f0b406500"
      },
      "source": [
        "### Loading the SAM 2 video predictor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f5f3245e-b4d6-418b-a42a-a67e0b3b5aec",
      "metadata": {
        "id": "f5f3245e-b4d6-418b-a42a-a67e0b3b5aec"
      },
      "outputs": [],
      "source": [
        "from sam2.build_sam import build_sam2_video_predictor\n",
        "\n",
        "sam2_checkpoint = \"../checkpoints/sam2.1_hiera_large.pt\"\n",
        "model_cfg = \"configs/sam2.1/sam2.1_hiera_l.yaml\"\n",
        "\n",
        "predictor = build_sam2_video_predictor(model_cfg, sam2_checkpoint, device=device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1a5320fe-06d7-45b8-b888-ae00799d07fa",
      "metadata": {
        "id": "1a5320fe-06d7-45b8-b888-ae00799d07fa"
      },
      "outputs": [],
      "source": [
        "def show_mask(mask, ax, obj_id=None, random_color=False):\n",
        "    if random_color:\n",
        "        color = np.concatenate([np.random.random(3), np.array([0.6])], axis=0)\n",
        "    else:\n",
        "        cmap = plt.get_cmap(\"tab10\")\n",
        "        cmap_idx = 0 if obj_id is None else obj_id\n",
        "        color = np.array([*cmap(cmap_idx)[:3], 0.6])\n",
        "    h, w = mask.shape[-2:]\n",
        "    mask_image = mask.reshape(h, w, 1) * color.reshape(1, 1, -1)\n",
        "    ax.imshow(mask_image)\n",
        "\n",
        "\n",
        "def show_points(coords, labels, ax, marker_size=200):\n",
        "    pos_points = coords[labels==1]\n",
        "    neg_points = coords[labels==0]\n",
        "    ax.scatter(pos_points[:, 0], pos_points[:, 1], color='green', marker='*', s=marker_size, edgecolor='white', linewidth=1.25)\n",
        "    ax.scatter(neg_points[:, 0], neg_points[:, 1], color='red', marker='*', s=marker_size, edgecolor='white', linewidth=1.25)\n",
        "\n",
        "\n",
        "def show_box(box, ax):\n",
        "    x0, y0 = box[0], box[1]\n",
        "    w, h = box[2] - box[0], box[3] - box[1]\n",
        "    ax.add_patch(plt.Rectangle((x0, y0), w, h, edgecolor='green', facecolor=(0, 0, 0, 0), lw=2))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f22aa751-b7cd-451e-9ded-fb98bf4bdfad",
      "metadata": {
        "id": "f22aa751-b7cd-451e-9ded-fb98bf4bdfad"
      },
      "source": [
        "#### Select an example video"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1c4c6af6-e18d-4939-beaf-2bc00f94a724",
      "metadata": {
        "id": "1c4c6af6-e18d-4939-beaf-2bc00f94a724"
      },
      "source": [
        "We assume that the video is stored as a list of JPEG frames with filenames like `<frame_index>.jpg`.\n",
        "\n",
        "For your custom videos, you can extract their JPEG frames using ffmpeg (https://ffmpeg.org/) as follows:\n",
        "```\n",
        "ffmpeg -i <your_video>.mp4 -q:v 2 -start_number 0 <output_dir>/'%05d.jpg'\n",
        "```\n",
        "where `-q:v` generates high-quality JPEG frames and `-start_number 0` asks ffmpeg to start the JPEG file from `00000.jpg`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "KtQoAuxI9CgP",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KtQoAuxI9CgP",
        "outputId": "8c96cfa8-0675-4d71-b8b6-619a70d62594"
      },
      "outputs": [],
      "source": [
        "!pwd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dsqTWeOG9CmG",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dsqTWeOG9CmG",
        "outputId": "b233ef42-2eac-47e4-d3f8-e062348fa873"
      },
      "outputs": [],
      "source": [
        "%cd videos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "sFf3U1IxAbIH",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sFf3U1IxAbIH",
        "outputId": "cf001f95-6c31-44f2-c190-adb6b813f20e"
      },
      "outputs": [],
      "source": [
        "!ls"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "pFnuAE0484X7",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "id": "pFnuAE0484X7",
        "outputId": "08c7aae4-5ee9-478a-e310-52838b19372d"
      },
      "outputs": [],
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fZqsGOjj_fUv",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fZqsGOjj_fUv",
        "outputId": "18e360b5-5428-4367-d140-ca49f4b2534f"
      },
      "outputs": [],
      "source": [
        "!ls"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "IAuGIj2v84bS",
      "metadata": {
        "id": "IAuGIj2v84bS"
      },
      "outputs": [],
      "source": [
        "import zipfile\n",
        "import io\n",
        "\n",
        "zf = zipfile.ZipFile(io.BytesIO(uploaded['octopus.zip']), \"r\")\n",
        "zf.extractall()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "KR7HOK-A84dd",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KR7HOK-A84dd",
        "outputId": "d94f94ee-e81b-46c7-9224-6b310ca44919"
      },
      "outputs": [],
      "source": [
        "!rm dinosaur.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8pmtTxPvARnh",
      "metadata": {
        "id": "8pmtTxPvARnh"
      },
      "outputs": [],
      "source": [
        "!rm -rf __MACOSX"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "qBcfW_gTBzo2",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qBcfW_gTBzo2",
        "outputId": "5b3d8bff-b5e9-4e9b-fb8a-62189c944e8f"
      },
      "outputs": [],
      "source": [
        "!ls"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "oPM20NscBztF",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oPM20NscBztF",
        "outputId": "8748be3a-1e42-417e-e4a1-238ce8321031"
      },
      "outputs": [],
      "source": [
        "%cd .."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b94c87ca-fd1a-4011-9609-e8be1cbe3230",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 492
        },
        "id": "b94c87ca-fd1a-4011-9609-e8be1cbe3230",
        "outputId": "08d435e3-141d-4baf-ae19-e72a4fff4b24"
      },
      "outputs": [],
      "source": [
        "# `video_dir` a directory of JPEG frames with filenames like `<frame_index>.jpg`\n",
        "video_dir = \"./videos/octopus\"\n",
        "\n",
        "# scan all the JPEG frame names in this directory\n",
        "frame_names = [\n",
        "    p for p in os.listdir(video_dir)\n",
        "    if os.path.splitext(p)[-1] in [\".jpg\", \".jpeg\", \".JPG\", \".JPEG\"]\n",
        "]\n",
        "frame_names.sort(key=lambda p: int(os.path.splitext(p)[0]))\n",
        "\n",
        "# take a look the first video frame\n",
        "frame_idx = 0\n",
        "plt.figure(figsize=(9, 6))\n",
        "plt.title(f\"frame {frame_idx}\")\n",
        "plt.imshow(Image.open(os.path.join(video_dir, frame_names[frame_idx])))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dff46b10-c17a-4a26-8004-8c6d80806b0a",
      "metadata": {
        "id": "dff46b10-c17a-4a26-8004-8c6d80806b0a"
      },
      "source": [
        "#### Initialize the inference state"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f594ac71-a6b9-461d-af27-500fa1d1a420",
      "metadata": {
        "id": "f594ac71-a6b9-461d-af27-500fa1d1a420"
      },
      "source": [
        "SAM 2 requires stateful inference for interactive video segmentation, so we need to initialize an **inference state** on this video.\n",
        "\n",
        "During initialization, it loads all the JPEG frames in `video_path` and stores their pixels in `inference_state` (as shown in the progress bar below)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8967aed3-eb82-4866-b8df-0f4743255c2c",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8967aed3-eb82-4866-b8df-0f4743255c2c",
        "outputId": "ab42f63a-b285-42b3-c80d-b1f206c65d71"
      },
      "outputs": [],
      "source": [
        "inference_state = predictor.init_state(video_path=video_dir)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2502bb5a-3e1f-43d0-9f58-33f8676fff0d",
      "metadata": {
        "id": "2502bb5a-3e1f-43d0-9f58-33f8676fff0d"
      },
      "source": [
        "### Segment an object using box prompt"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8e2d26c8-0432-48c6-997e-4a3b77bb5f6d",
      "metadata": {
        "id": "8e2d26c8-0432-48c6-997e-4a3b77bb5f6d"
      },
      "source": [
        "Note: if you have run any previous tracking using this `inference_state`, please reset it first via `reset_state`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6dbe9183-abbb-4283-b0cb-d24f3d7beb34",
      "metadata": {
        "id": "6dbe9183-abbb-4283-b0cb-d24f3d7beb34"
      },
      "outputs": [],
      "source": [
        "predictor.reset_state(inference_state)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ceb6eae9-0f4c-434f-8089-a46c9ca59da5",
      "metadata": {
        "id": "ceb6eae9-0f4c-434f-8089-a46c9ca59da5"
      },
      "source": [
        "In addition to using clicks as inputs, SAM 2 also supports segmenting and tracking objects in a video via **bounding boxes**.\n",
        "\n",
        "In the example below, we segment the child on the right using a **box prompt** of (x_min, y_min, x_max, y_max) = (300, 0, 500, 400) on frame 0 as input into the `add_new_points_or_box` API."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1cbfb273-4e14-495b-bd89-87a8baf52ae7",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 564
        },
        "id": "1cbfb273-4e14-495b-bd89-87a8baf52ae7",
        "outputId": "50051ffa-8c33-4082-eaa6-03836e813415"
      },
      "outputs": [],
      "source": [
        "ann_frame_idx = 0  # the frame index we interact with\n",
        "ann_obj_id = 4  # give a unique id to each object we interact with (it can be any integers)\n",
        "\n",
        "# Let's add a box at (x_min, y_min, x_max, y_max) = (300, 0, 500, 400) to get started\n",
        "#box = np.array([300, 0, 500, 400], dtype=np.float32)\n",
        "#box = np.array([169, 389, 1567, 968], dtype=np.float32)\n",
        "box = np.array([797, 353, 1290, 832], dtype=np.float32)\n",
        "_, out_obj_ids, out_mask_logits = predictor.add_new_points_or_box(\n",
        "    inference_state=inference_state,\n",
        "    frame_idx=ann_frame_idx,\n",
        "    obj_id=ann_obj_id,\n",
        "    box=box,\n",
        ")\n",
        "\n",
        "# show the results on the current (interacted) frame\n",
        "plt.figure(figsize=(9, 6))\n",
        "plt.title(f\"frame {ann_frame_idx}\")\n",
        "plt.imshow(Image.open(os.path.join(video_dir, frame_names[ann_frame_idx])))\n",
        "show_box(box, plt.gca())\n",
        "show_mask((out_mask_logits[0] > 0.0).cpu().numpy(), plt.gca(), obj_id=out_obj_ids[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bd3f9ba7-bf4d-47e5-9b02-8a424cab42cc",
      "metadata": {
        "id": "bd3f9ba7-bf4d-47e5-9b02-8a424cab42cc"
      },
      "source": [
        "Here, SAM 2 gets a pretty good segmentation mask of the entire child, even though the input bounding box is not perfectly tight around the object.\n",
        "\n",
        "Similar to the previous example, if the returned mask from is not perfect when using a box prompt, we can also further **refine** the output using positive or negative clicks. To illustrate this, here we make a **positive click** at (x, y) = (460, 60) with label `1` to expand the segment around the child's hair.\n",
        "\n",
        "Note: to refine the segmentation mask from a box prompt, we need to send **both the original box input and all subsequent refinement clicks and their labels** when calling `add_new_points_or_box`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "54906315-ab4c-4088-b866-4c22134d5b66",
      "metadata": {
        "id": "54906315-ab4c-4088-b866-4c22134d5b66"
      },
      "outputs": [],
      "source": [
        "# For fullly automatic method, we skip this part\n",
        "# For dinosaur demo, this was done, not for octopus\n",
        "\n",
        "# ann_frame_idx = 0  # the frame index we interact with\n",
        "# ann_obj_id = 4  # give a unique id to each object we interact with (it can be any integers)\n",
        "\n",
        "# # Let's add a positive click at (x, y) = (460, 60) to refine the mask\n",
        "# #points = np.array([[460, 60]], dtype=np.float32)\n",
        "# points = np.array([[210, 480]], dtype=np.float32)\n",
        "# # for labels, `1` means positive click and `0` means negative click\n",
        "# labels = np.array([1], np.int32)\n",
        "# # note that we also need to send the original box input along with\n",
        "# # the new refinement click together into `add_new_points_or_box`\n",
        "# #box = np.array([300, 0, 500, 400], dtype=np.float32)\n",
        "# box = np.array([169, 389, 1567, 968], dtype=np.float32)\n",
        "# _, out_obj_ids, out_mask_logits = predictor.add_new_points_or_box(\n",
        "#     inference_state=inference_state,\n",
        "#     frame_idx=ann_frame_idx,\n",
        "#     obj_id=ann_obj_id,\n",
        "#     points=points,\n",
        "#     labels=labels,\n",
        "#     box=box,\n",
        "# )\n",
        "\n",
        "# # show the results on the current (interacted) frame\n",
        "# plt.figure(figsize=(9, 6))\n",
        "# plt.title(f\"frame {ann_frame_idx}\")\n",
        "# plt.imshow(Image.open(os.path.join(video_dir, frame_names[ann_frame_idx])))\n",
        "# show_box(box, plt.gca())\n",
        "# show_points(points, labels, plt.gca())\n",
        "# show_mask((out_mask_logits[0] > 0.0).cpu().numpy(), plt.gca(), obj_id=out_obj_ids[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "73128cd6-dbfa-49f7-8d79-1a8e19835f7f",
      "metadata": {
        "id": "73128cd6-dbfa-49f7-8d79-1a8e19835f7f"
      },
      "source": [
        "Then, to get the masklet throughout the entire video, we propagate the prompts using the `propagate_in_video` API."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9cd90557-a0dc-442e-b091-9c74c831bef8",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "9cd90557-a0dc-442e-b091-9c74c831bef8",
        "outputId": "e6d8640b-bdc9-4459-d44b-d5deb0de4dfd"
      },
      "outputs": [],
      "source": [
        "# run propagation throughout the video and collect the results in a dict\n",
        "video_segments = {}  # video_segments contains the per-frame segmentation results\n",
        "for out_frame_idx, out_obj_ids, out_mask_logits in predictor.propagate_in_video(inference_state):\n",
        "    video_segments[out_frame_idx] = {\n",
        "        out_obj_id: (out_mask_logits[i] > 0.0).cpu().numpy()\n",
        "        for i, out_obj_id in enumerate(out_obj_ids)\n",
        "    }\n",
        "\n",
        "# render the segmentation results every few frames\n",
        "vis_frame_stride = 30\n",
        "plt.close(\"all\")\n",
        "for out_frame_idx in range(0, len(frame_names), vis_frame_stride):\n",
        "    plt.figure(figsize=(6, 4))\n",
        "    plt.title(f\"frame {out_frame_idx}\")\n",
        "    plt.imshow(Image.open(os.path.join(video_dir, frame_names[out_frame_idx])))\n",
        "    for out_obj_id, out_mask in video_segments[out_frame_idx].items():\n",
        "        show_mask(out_mask, plt.gca(), obj_id=out_obj_id)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "O9fgG4YpE80N",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O9fgG4YpE80N",
        "outputId": "56f32f8c-36b6-440c-9d6b-f6c0e29142fa"
      },
      "outputs": [],
      "source": [
        "from PIL import Image, ImageDraw\n",
        "from tqdm import tqdm\n",
        "\n",
        "output_dir = \"./videos/results\"\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "for out_frame_idx in tqdm(range(0, len(frame_names))):\n",
        "\n",
        "    # Load image\n",
        "    orig_path = os.path.join(video_dir, frame_names[out_frame_idx])\n",
        "    orig_img = Image.open(orig_path).convert(\"RGBA\")\n",
        "\n",
        "    # Create a blank RGBA image for the overlay\n",
        "    overlay = Image.new(\"RGBA\", orig_img.size, (0, 0, 0, 0))\n",
        "\n",
        "    for out_obj_id, out_mask in video_segments[out_frame_idx].items():\n",
        "        mask_2d = np.squeeze(out_mask)\n",
        "        alpha = (mask_2d * 100).astype(np.uint8)\n",
        "        color = (255, 0, 0, 0)\n",
        "\n",
        "        color_mask = Image.fromarray(np.stack([\n",
        "            np.full_like(alpha, color[0]),\n",
        "            np.full_like(alpha, color[1]),\n",
        "            np.full_like(alpha, color[2]),\n",
        "            alpha,\n",
        "        ], axis=-1), mode=\"RGBA\")\n",
        "\n",
        "        overlay = Image.alpha_composite(overlay, color_mask)\n",
        "\n",
        "    # merge image and mask\n",
        "    combined = Image.alpha_composite(orig_img, overlay)\n",
        "\n",
        "    # Save output\n",
        "    combined.convert(\"RGB\").save(os.path.join(output_dir, f\"{out_frame_idx:05d}.png\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0OpA4iJbK4Wj",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0OpA4iJbK4Wj",
        "outputId": "a2af4405-373c-45ca-ee73-569a067503eb"
      },
      "outputs": [],
      "source": [
        "!ffmpeg -framerate 30 -i ./videos/results/%05d.png -c:v libx264 -pix_fmt yuv420p output.mp4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "GvAluBqcLBcu",
      "metadata": {
        "id": "GvAluBqcLBcu"
      },
      "outputs": [],
      "source": [
        "# ffmpeg -i octopus.mp4 -i octopus_output.mp4 -filter_complex \\\n",
        "# \"[0:v]fps=24,scale=1920:1080[left];[1:v]fps=24,scale=1920:1080[right];[left][right]hstack=inputs=2\" \\\n",
        "# -c:v libx264 -crf 23 octopus_merged.mp4\n",
        "\n",
        "\n",
        "# ffmpeg -i left.mp4 -i right.mp4 -filter_complex \\\n",
        "# \"[0:v][1:v]hstack=inputs=2[stacked]; \\\n",
        "#  [stacked]pad=iw:ih+80:0:0:black[padded]; \\\n",
        "#  [padded]drawtext=text='Left caption':x=(w/4-text_w/2):y=h-60:fontsize=32:fontcolor=white, \\\n",
        "#          drawtext=text='Right caption':x=(3*w/4-text_w/2):y=h-60:fontsize=32:fontcolor=white\" \\\n",
        "# -c:v libx264 -crf 18 -preset veryfast output.mp4\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "oXazpm1DLBfT",
      "metadata": {
        "id": "oXazpm1DLBfT"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Rvj3VmBzJrvh",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rvj3VmBzJrvh",
        "outputId": "f4e57c15-42e1-4c5f-9c9f-e5a89525d876"
      },
      "outputs": [],
      "source": [
        "!ls"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "hPFrSQ_aJryH",
      "metadata": {
        "id": "hPFrSQ_aJryH"
      },
      "outputs": [],
      "source": [
        "!rm -rf videos/results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7a6rets3Jr1q",
      "metadata": {
        "id": "7a6rets3Jr1q"
      },
      "outputs": [],
      "source": [
        "!rm output.mp4"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e023f91f-0cc5-4980-ae8e-a13c5749112b",
      "metadata": {
        "id": "e023f91f-0cc5-4980-ae8e-a13c5749112b"
      },
      "source": [
        "Note that in addition to clicks or boxes, SAM 2 also supports directly using a **mask prompt** as input via the `add_new_mask` method in the `SAM2VideoPredictor` class. This can be helpful in e.g. semi-supervised VOS evaluations (see [tools/vos_inference.py](https://github.com/facebookresearch/sam2/blob/main/tools/vos_inference.py) for an example)."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.14"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
