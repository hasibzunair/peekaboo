<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <title>Title</title>
</head>
<body> 
    Try this demo for <a href="https://github.com/hasibzunair/peekaboo">PEEKABOO</a>, 
    introduced in our <strong>BMVC'2024</strong> paper <a href="https://arxiv.org/abs/2407.17628">PEEKABOO: Hiding Parts of an Image for Unsupervised Object Localization</a>.
    </br>
    Peekaboo aims to explicitly model contextual relationship among pixels through image masking for unsupervised object localization. 
    In a self-supervised procedure (i.e. pretext task) without any additional training (i.e. downstream task), context-based representation learning is done at both 
    the pixel-level by making predictions on masked images and at shape-level by matching the predictions of the masked input to the unmasked one.
    </br>
    You can use this demo to segment the most salient as well as novel object(s) in your images. To use it, simply 
    upload an image of your choice and hit submit. You will get one or more segmentation maps of the most salient objects present 
    in your images.
    </br>
    <a href="https://hasibzunair.github.io/peekaboo/"><strong>Project Page</strong></a>
    </br>
</body>
</html>