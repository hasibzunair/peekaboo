<script src="http://www.google.com/jsapi" type="text/javascript"></script> 
<script type="text/javascript">google.load("jquery", "1.3.2");</script>

<style type="text/css">
	body {
		font-family: "HelveticaNeue-Light", "Helvetica Neue Light", "Helvetica Neue", Helvetica, Arial, "Lucida Grande", sans-serif; 
		font-weight:300;
		font-size:18px;
		margin-left: auto;
		margin-right: auto;
		width: 1100px;
	}
	
	h1 {
		font-size:32px;
		font-weight:300;
	}
	
	.disclaimerbox {
		background-color: #eee;		
		border: 1px solid #eeeeee;
		border-radius: 10px ;
		-moz-border-radius: 10px ;
		-webkit-border-radius: 10px ;
		padding: 20px;
	}

	video.header-vid {
		height: 140px;
		border: 1px solid black;
		border-radius: 10px ;
		-moz-border-radius: 10px ;
		-webkit-border-radius: 10px ;
	}
	
	img.header-img {
		height: 140px;
		border: 1px solid black;
		border-radius: 10px ;
		-moz-border-radius: 10px ;
		-webkit-border-radius: 10px ;
	}
	
	img.rounded {
		border: 1px solid #eeeeee;
		border-radius: 10px ;
		-moz-border-radius: 10px ;
		-webkit-border-radius: 10px ;
	}
	
	a:link,a:visited
	{
		color: #1367a7;
		text-decoration: none;
	}
	a:hover {
		color: #208799;
	}
	
	td.dl-link {
		height: 160px;
		text-align: center;
		font-size: 22px;
	}
	
	.layered-paper-big { /* modified from: http://css-tricks.com/snippets/css/layered-paper/ */
		box-shadow:
		0px 0px 1px 1px rgba(0,0,0,0.35), /* The top layer shadow */
		5px 5px 0 0px #fff, /* The second layer */
		5px 5px 1px 1px rgba(0,0,0,0.35), /* The second layer shadow */
		10px 10px 0 0px #fff, /* The third layer */
		10px 10px 1px 1px rgba(0,0,0,0.35), /* The third layer shadow */
		15px 15px 0 0px #fff, /* The fourth layer */
		15px 15px 1px 1px rgba(0,0,0,0.35), /* The fourth layer shadow */
		20px 20px 0 0px #fff, /* The fifth layer */
		20px 20px 1px 1px rgba(0,0,0,0.35), /* The fifth layer shadow */
		25px 25px 0 0px #fff, /* The fifth layer */
		25px 25px 1px 1px rgba(0,0,0,0.35); /* The fifth layer shadow */
		margin-left: 10px;
		margin-right: 45px;
	}

	.paper-big { /* modified from: http://css-tricks.com/snippets/css/layered-paper/ */
		box-shadow:
		0px 0px 1px 1px rgba(0,0,0,0.35); /* The top layer shadow */

		margin-left: 10px;
		margin-right: 45px;
	}


	.layered-paper { /* modified from: http://css-tricks.com/snippets/css/layered-paper/ */
		box-shadow:
		0px 0px 1px 1px rgba(0,0,0,0.35), /* The top layer shadow */
		5px 5px 0 0px #fff, /* The second layer */
		5px 5px 1px 1px rgba(0,0,0,0.35), /* The second layer shadow */
		10px 10px 0 0px #fff, /* The third layer */
		10px 10px 1px 1px rgba(0,0,0,0.35); /* The third layer shadow */
		margin-top: 5px;
		margin-left: 10px;
		margin-right: 30px;
		margin-bottom: 5px;
	}
	
	.vert-cent {
		position: relative;
		top: 50%;
		transform: translateY(-50%);
	}
	
	hr
	{
		border: 0;
		height: 1px;
		background-image: linear-gradient(to right, rgba(0, 0, 0, 0), rgba(0, 0, 0, 0.75), rgba(0, 0, 0, 0));
	}
</style>

<html>
<head>
	<title>Peekaboo for Unsupervised Object Localization</title>
	<meta property="og:image" content="./resources/model.png"/> <!-- Facebook automatically scrapes this. Go to https://developers.facebook.com/tools/debug/ if you update and want to force Facebook to rescrape. -->
	<meta property="og:title" content="PEEKABOO: Hiding Parts of an Image for Unsupervised Object Localization" />
	<meta property="og:description" content="Paper description." />

	<!-- Get from Google Analytics -->
	<!-- Global site tag (gtag.js) - Google Analytics -->
	<script async src=""></script> 
	<script>
		window.dataLayer = window.dataLayer || [];
		function gtag(){dataLayer.push(arguments);}
		gtag('js', new Date());

		gtag('config', 'UA-75863369-6');
	</script>
</head>

<body>
	<br>
	<center>
		<span style="font-size:36px">PEEKABOO: Hiding Parts of an Image for Unsupervised Object Localization</span>
		<table align=center width=600px>
			<table align=center width=450px> <!--Adjust for spacing between author names-->
				<tr>
					<!--Add author names here.-->
					<td align=center width=100px>
						<center>
							<span style="font-size:24px"><a href="http://hasibzunair.github.io/">Hasib Zunair</a><sup>1</sup></span>
						</center>
					</td>
					<td align=center width=100px>
						<center>
							<span style="font-size:24px"><a href="https://users.encs.concordia.ca/~hamza/">A. Ben Hamza</a><sup>1</sup></span>
						</center>
					</td>
					<!--
					<td align=center width=100px>
						<center>
							<span style="font-size:24px"><a href="https://en.wikipedia.org/wiki/James_J._Gibson">Second Author</a></span>
						</center>
					</td>
					<td align=center width=100px>
						<center>
							<span style="font-size:24px"><a href="https://en.wikipedia.org/wiki/James_J._Gibson">Third Author</a></span>
						</center>
					</td>
					-->
				</tr>
			</table>

			<!--Add affiliations here.-->
			<table align="center" width="700px">
				<tbody>
					<tr>
						<td align="center" width="200px">
							<center>
								<span style="font-size:24px"><sup>1</sup>Concordia University, Montreal, QC, Canada.</span>
							</center>
						</td>
						<!--
						<td align="center" width="200px">
							<center>
							<span style="font-size:20px"><sup>2</sup>University X</span>
							</center>
						</td>
						-->
			 		</tr>
				</tbody>
			</table>

			<!--Add publication details here.-->
			<table align="center" width="400px">
				<tbody>
					<tr>
						<td align="center" width="100px">
							<center>
								<span style="font-size:25px; color: crimson;">British Machine Vision Conference BMVC 2024</span>
							</center>
							<!--
							<center>
								<span style="font-size:20px">Digital Geometric Modelling</span>
							</center>
							-->
						</td>
					</tr>
				</tbody>
			</table>

			<!--Add supplementary links here.-->
			<table align=center width=500px>
				<tr>
					
					<td align=center width=80px>
						<center>
							<span style="font-size:24px"><a href='https://huggingface.co/spaces/hasibzunair/peekaboo-demo'>[Demo]</a></span><br>
						</center>
					</td>
					
					<td align=center width=80px>
						<center>
							<span style="font-size:24px"><a href='https://github.com/hasibzunair/peekaboo'>[GitHub]</a></span><br>
						</center>
					</td>
					
					<td align=center width=80px>
						<center>
							<span style="font-size:24px"><a href='https://arxiv.org/abs/2407.17628'>[Paper]</a></span>
						</center>
					</td>

					<!--  
					<td align=center width=80px>
						<center>
							<span style="font-size:24px"><a href='TBA'>[Video]</a></span>
						</center>
					</td>


					<td align=center width=80px>
						<center>
							<span style="font-size:24px"><a href='TBA'>[Poster]</a></span>
						</center>
					</td>
					-->

					
					<!--  
					<td align=center width=120px>
						<center>
							<span style="font-size:24px"><a href=''>[Talk]</a></span><br>
						</center>
					</td>
					
					<td align=center width=120px>
						<center>
							<span style="font-size:24px"><a href=''>[Slides]</a></span><br>
						</center>
					</td>
					
					<td align=center width=120px>
						<center>
							<span style="font-size:24px"><a href=''>[Poster]</a></span><br>
						</center>
					</td>
					-->
				</tr>
			</table>
		</table>
	</center>

	<br>
	
	<center> 
		<table align=center width=850px>
			<tr>
				<td width=260px>
					<center>
						<!--Add intro image here.-->
						<img class="round" style="width:850px" src="./resources/compare_predictions.jpg"/>
						
					</center>
				</td>
			</tr>

			<tr>
				<td width="600px">
					<center>
						<span style="font-size:16px"><i><strong> 
							Visual comparison of PEEKABOO and state-of-the-art FOUND on ECSSD, DUT-OMRON and DUTS-TE datasets</strong>. Across all datasets, PEEKABOO excels
							in localizing salient objects, particularly when they are <strong>small</strong>, <strong>reflective</strong>, or when background is 
							<strong>poorly illuminated</strong>. PEEKABOO also avoids <strong>over-segmenting salient objects</strong>, <strong>segmenting non-salient regions</strong>, 
							and <strong>producing noisy predictions</strong>. Zoom in to observe the results more closely.</i>
						</span>
					</center>
				</td>
			</tr>
		</table>
	</center>
	
	<br>

	<strong>TL;DR</strong>: A segmentation model with zero-shot generalization to unfamiliar images and objects that are small, reflective or under poor 
	illumination without the need for additional training. Our approach aims to explicitly model contextual relationship among 
	pixels in a self-supervised procedure through image masking for unsupervised object localization.

	<br><br>
	<hr>
	<br>

		<div class="disclaimerbox">
		  	<!-- <center><h2>How to interpret the results</h2></center> -->

		 	<span style="color:#646464">
			  	
				<center><span style="font-size:28px"><b>How to interpret the results</b></span></center>
			
				<br>
		  
				<i> 
					While unsupervised object localization algorithms work well on many cases, they often fail to segment salient objects 
					that are small, reflective or when the background is poorly illuminated. These real-world characterstics result in methods to 
					over-segment, segment non-salient regions and produce noisy segmentation maps. 
					Even though this is not a solved problem, quite far from it, 
					we believe our work is a significant step forward in solving 
					unsupervised object localization in the aforementioned scenarios. 
					There has been some concurrent work on this subject as well. 
					Specifically, see <a href="https://github.com/valeoai/FOUND">FOUND</a>.
				</i>

				<br><br>

				<i> 
					Our approach aims to explicitly model contextual relationship among pixels through image masking 
					for unsupervised object localization. In a self-supervised procedure (i.e. pretext task) without any additional training 
					(i.e. downstream task), context-based representation learning is done at both the pixel-level by making predictions on masked 
					images and at shape-level by matching the predictions of the masked input to the unmasked one.
				</i>

				<br><br>

				<i>
					We explored this technique in supervised settings in our previous works. Specifically, for <a href="https://openaccess.thecvf.com/content/WACV2024/html/Zunair_Learning_To_Recognize_Occluded_and_Small_Objects_With_Partial_Inputs_WACV_2024_paper.html"><strong>recognition</strong></a> 
					of small and occluded objects (WACV 2024) and for <a href="https://arxiv.org/abs/2210.00923"><strong>segmentation</strong></a> of ambiguous regions and is shape aware while being simple and compute efficient (BMVC 2022, Oral).
				</i>

				<br><br>

				<i>Do enjoy our results, and definitely <a href="https://huggingface.co/spaces/hasibzunair/peekaboo-demo">try the model</a> yourself</i>!
	  		</span>
	  	</div>

	<br>
	<hr>

	<center><h1>Results on unfamiliar images and objects</h1></center>
	We show some results of Peekaboo on unfamiliar objects that are out-of-domain of ImageNet and DUT-TR, of different scales and shapes which 
	are correctly localized. Specifically, octopus, dinosaurs and spaceships are not present in ImageNet/DUT-TR, Peekaboo can still detect them. 
	These results show the ability of Peekaboo to discover multiple and diverse objects, basically which "are not background".

	<br><br>

	<table align=center width=400px>
			<td align=center width=400px>
				<center>
					<td><img class="round" style="width:1000px" src="./resources/images.png"/></td>
				</center>
			</td>
		</tr>
	</table>

	<br>
	<hr>

	<center><h1>Try the Interactive Demo</h1></center>
	
	<table align=center width=400px>
		<tr>
			<td align=center width=400px>
			<center>
				<td><a href='https://huggingface.co/spaces/hasibzunair/peekaboo-demo'><img class="round" style="width:800px" src="./resources/spaces.png"/></a></td>
			</center>
	  </tr>
  </table>

  	<table align=center width=100px>
		<tr>
			<center> <br>
	  		<span style="font-size:28px"><a href='https://huggingface.co/spaces/hasibzunair/peekaboo-demo'>[🤗 Spaces]</a> Demo</span></span><i></i>
			</center></tr>
	</table>

	<br>
	<hr>

	<table align=center width=900px>
		<center><h1>Abstract</h1></center>
		<tr>
			<td>
				Localizing objects in an unsupervised manner poses significant challenges due to the absence of key visual 
				information such as the appearance, type and number of objects, as well as the lack of labeled object classes 
				typically available in supervised settings. While recent approaches to unsupervised object localization have 
				demonstrated significant progress by leveraging self-supervised visual representations, they often <FONT COLOR="#ff0000">require 
				computationally intensive training processes, resulting in high resource demands in terms of computation, 
				learnable parameters, and data</FONT>. They also <FONT COLOR="#ff0000">lack explicit modeling of visual context, potentially limiting 
				their accuracy in object localization</FONT>. To tackle these challenges, <strong>we propose a single-stage learning framework, 
				dubbed PEEKABOO, for unsupervised object localization</strong> by learning context-based representations at both the 
				pixel- and shape-level of the localized objects through image masking. The key idea is to selectively hide parts 
				of an image and leverage the remaining image information to infer the location of objects without explicit supervision. 
				The experimental results, both quantitative and qualitative, across various benchmark datasets, <FONT COLOR="#46C646">demonstrate the simplicity, 
				effectiveness and competitive performance</FONT> of our approach compared to state-of-the-art methods in both single object 
				discovery and unsupervised salient object detection tasks.
			</td>
		</tr>
	</table>
	
	<br>
	<hr>

	<!--Add YouTube talk here.-->
	<!--
	<center><h1>Talk</h1></center>
	<p align="center">
		<iframe width="660" height="395" src="https://www.youtube.com/embed/dQw4w9WgXcQ" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen align="center"></iframe>
	</p>

	<table align=center width=800px>
		<br>
		<tr>
			<center>
				<span style="font-size:28px"><a href=''>[Slides]</a>
				</span>
			</center>
		</tr>
	</table>
	<br>
	<hr>
	-->

	<!--Add demo code here with architecture.-->
	<center><h1>Our Method</h1></center>
	<table align=center width=400px>
		<tr>
			<td align=center width=400px>
				<center>
					<td><a href='https://github.com/hasibzunair/peekaboo'><img class="round" style="width:800px" src="./resources/figure.jpg"/></a></td>
				</center>
			</td>
		</tr>
	</table>

	<table align=center width=100px>
		<tr>
			<center> <br>
	  		<span style="font-size:28px"><a href='https://github.com/hasibzunair/peekaboo'>[GitHub]</a> Code</span></span><i></i>
			</center></tr>
	</table>

	<br>
	<hr>

	<table align=center width=500px>
		<center><h1>Paper and Supplementary Material</h1></center>
		<tr>
			<td><a href="https://arxiv.org/abs/2407.17628"><img class="layered-paper-big" style="height:175px" src="./resources/paper.png"/></a></td>
			<td><span style="font-size:14pt">Hasib Zunair and A. Ben Hamza<br>
				<b>PEEKABOO: Hiding Parts of an Image for Unsupervised Object Localization.</b><br>
				In BMVC, 2024.<br>
				<!--Add arXiv link-->
				(hosted on <a href="https://arxiv.org/abs/2407.17628">ArXiv</a>)<br>
				</span>
			</td>
		</tr>
	</table>
	<br>

	<!--Add Bibtex-->
	<!--
	<table align=center width=600px>
		<tr>
			<td><span style="font-size:14pt"><center>
				<a href="./resources/bibtex.txt">[Bibtex]</a>
			</center></td>
		</tr>
	</table>
	-->

	<br>
	<hr>

	<table align=center width=900px>
		<tr>
			<td width=400px>
				<left>
					<center><h1>Acknowledgements</h1></center>
					<center>This website template was originally made by <a href="http://web.mit.edu/phillipi/">Phillip Isola</a> and <a href="http://richzhang.github.io/">Richard Zhang</a>, code can be found <a href="https://github.com/richzhang/webpage-template">here</a>.</center>
				</left>
			</td>
		</tr>
	</table>
<br>

</body>
</html>